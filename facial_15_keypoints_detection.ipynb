{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import time\n",
    "import pandas as pd\n",
    "import keras\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "left_eye_center_x            7039\n",
      "left_eye_center_y            7039\n",
      "right_eye_center_x           7036\n",
      "right_eye_center_y           7036\n",
      "left_eye_inner_corner_x      2271\n",
      "left_eye_inner_corner_y      2271\n",
      "left_eye_outer_corner_x      2267\n",
      "left_eye_outer_corner_y      2267\n",
      "right_eye_inner_corner_x     2268\n",
      "right_eye_inner_corner_y     2268\n",
      "right_eye_outer_corner_x     2268\n",
      "right_eye_outer_corner_y     2268\n",
      "left_eyebrow_inner_end_x     2270\n",
      "left_eyebrow_inner_end_y     2270\n",
      "left_eyebrow_outer_end_x     2225\n",
      "left_eyebrow_outer_end_y     2225\n",
      "right_eyebrow_inner_end_x    2270\n",
      "right_eyebrow_inner_end_y    2270\n",
      "right_eyebrow_outer_end_x    2236\n",
      "right_eyebrow_outer_end_y    2236\n",
      "nose_tip_x                   7049\n",
      "nose_tip_y                   7049\n",
      "mouth_left_corner_x          2269\n",
      "mouth_left_corner_y          2269\n",
      "mouth_right_corner_x         2270\n",
      "mouth_right_corner_y         2270\n",
      "mouth_center_top_lip_x       2275\n",
      "mouth_center_top_lip_y       2275\n",
      "mouth_center_bottom_lip_x    7016\n",
      "mouth_center_bottom_lip_y    7016\n",
      "Image                        7049\n",
      "dtype: int64\n",
      "X.shape == (2140, 9216); X.min == 0.000; X.max == 1.000\n",
      "y.shape == (2140, 30); y.min == -0.920; y.max == 0.996\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "from pandas.io.parsers import read_csv\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "\n",
    "FTRAIN = 'training.csv'\n",
    "FTEST = 'test.csv'\n",
    "\n",
    "\n",
    "def load(test=False, cols=None):\n",
    "    \"\"\"Loads data from FTEST if *test* is True, otherwise from FTRAIN.\n",
    "    Pass a list of *cols* if you're only interested in a subset of the\n",
    "    target columns.\n",
    "    \"\"\"\n",
    "    fname = FTEST if test else FTRAIN\n",
    "    df = read_csv(os.path.expanduser(fname))  # load pandas dataframe\n",
    "\n",
    "    # The Image column has pixel values separated by space; convert\n",
    "    # the values to numpy arrays:\n",
    "    df['Image'] = df['Image'].apply(lambda im: np.fromstring(im, sep=' '))\n",
    "\n",
    "    if cols:  # get a subset of columns\n",
    "        df = df[list(cols) + ['Image']]\n",
    "\n",
    "    print(df.count())  # prints the number of values for each column\n",
    "    df = df.dropna()  # drop all rows that have missing values in them\n",
    "\n",
    "    X = np.vstack(df['Image'].values) / 255.  # scale pixel values to [0, 1]\n",
    "    X = X.astype(np.float32)\n",
    "\n",
    "    if not test:  # only FTRAIN has any target columns\n",
    "        y = df[df.columns[:-1]].values\n",
    "        y = (y - 48) / 48  # scale target coordinates to [-1, 1]\n",
    "        X, y = shuffle(X, y, random_state=42)  # shuffle train data\n",
    "        y = y.astype(np.float32)\n",
    "    else:\n",
    "        y = None\n",
    "\n",
    "    return X, y\n",
    "\n",
    "\n",
    "X, y = load()\n",
    "print(\"X.shape == {}; X.min == {:.3f}; X.max == {:.3f}\".format(\n",
    "    X.shape, X.min(), X.max()))\n",
    "print(\"y.shape == {}; y.min == {:.3f}; y.max == {:.3f}\".format(\n",
    "    y.shape, y.min(), y.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "left_eye_center_x            7039\n",
      "left_eye_center_y            7039\n",
      "right_eye_center_x           7036\n",
      "right_eye_center_y           7036\n",
      "left_eye_inner_corner_x      2271\n",
      "left_eye_inner_corner_y      2271\n",
      "left_eye_outer_corner_x      2267\n",
      "left_eye_outer_corner_y      2267\n",
      "right_eye_inner_corner_x     2268\n",
      "right_eye_inner_corner_y     2268\n",
      "right_eye_outer_corner_x     2268\n",
      "right_eye_outer_corner_y     2268\n",
      "left_eyebrow_inner_end_x     2270\n",
      "left_eyebrow_inner_end_y     2270\n",
      "left_eyebrow_outer_end_x     2225\n",
      "left_eyebrow_outer_end_y     2225\n",
      "right_eyebrow_inner_end_x    2270\n",
      "right_eyebrow_inner_end_y    2270\n",
      "right_eyebrow_outer_end_x    2236\n",
      "right_eyebrow_outer_end_y    2236\n",
      "nose_tip_x                   7049\n",
      "nose_tip_y                   7049\n",
      "mouth_left_corner_x          2269\n",
      "mouth_left_corner_y          2269\n",
      "mouth_right_corner_x         2270\n",
      "mouth_right_corner_y         2270\n",
      "mouth_center_top_lip_x       2275\n",
      "mouth_center_top_lip_y       2275\n",
      "mouth_center_bottom_lip_x    7016\n",
      "mouth_center_bottom_lip_y    7016\n",
      "Image                        7049\n",
      "dtype: int64\n",
      "X_train.shape == (2140, 9216)\n",
      "y_train.shape == (2140, 30); y_train.min == -0.920; y_train.max == 0.996\n",
      "ImageId    1783\n",
      "Image      1783\n",
      "dtype: int64\n",
      "X_test.shape == (1783, 9216)\n"
     ]
    }
   ],
   "source": [
    "# Load training set\n",
    "X_train, y_train = load()\n",
    "print(\"X_train.shape == {}\".format(X_train.shape))\n",
    "print(\"y_train.shape == {}; y_train.min == {:.3f}; y_train.max == {:.3f}\".format(\n",
    "    y_train.shape, y_train.min(), y_train.max()))\n",
    "\n",
    "# Load testing set\n",
    "X_test, _ = load(test=True)\n",
    "print(\"X_test.shape == {}\".format(X_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "def plot_data(img, landmarks, axis):\n",
    "    axis.imshow(np.reshape(img,[96,96]), cmap='gray') # plot the image\n",
    "    landmarks = landmarks * 48 + 48 # undo the normalization\n",
    "    # Plot the keypoints\n",
    "    axis.scatter(landmarks[0::2], \n",
    "        landmarks[1::2], \n",
    "        marker='o', \n",
    "        c='c', \n",
    "        s=40)\n",
    "    \n",
    "fig = plt.figure(figsize=(20,20))\n",
    "fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)\n",
    "for i in range(9):\n",
    "    ax = fig.add_subplot(3, 3, i + 1, xticks=[], yticks=[])\n",
    "    plot_data(X_train[i], y_train[i], ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Anaconda__\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1290: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 96, 96, 4)         20        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 48, 48, 4)         0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 48, 48, 4)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 48, 48, 8)         136       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 24, 24, 8)         0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 24, 24, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 24, 24, 16)        528       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 12, 12, 16)        0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 12, 12, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 12, 12, 32)        2080      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 6, 6, 32)          0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 6, 6, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 6, 6, 64)          8256      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 3, 3, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 3, 3, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 3, 3, 128)         32896     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 1, 1, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 1, 1, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 1, 1, 256)         131328    \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_1 ( (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 30)                7710      \n",
      "=================================================================\n",
      "Total params: 445,866\n",
      "Trainable params: 445,866\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution2D, MaxPooling2D, Dropout, GlobalAveragePooling2D\n",
    "from keras.layers import Flatten, Dense\n",
    "\n",
    "\n",
    "## TODO: Specify a CNN architecture\n",
    "# Your model should accept 96x96 pixel graysale images in\n",
    "# It should have a fully-connected output layer with 30 values (2 for each facial keypoint)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Convolution2D(4, (2, 2), input_shape=(96,96,1),activation='relu', padding=\"same\"))\n",
    "model.add(MaxPooling2D(2))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Convolution2D(8, (2, 2),activation='relu', padding=\"same\"))\n",
    "model.add(MaxPooling2D(2))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Convolution2D(16, (2, 2),activation='relu', padding=\"same\"))\n",
    "model.add(MaxPooling2D(2))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Convolution2D(32, (2, 2),activation='relu', padding=\"same\"))\n",
    "model.add(MaxPooling2D(2))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Convolution2D(64, (2, 2),activation='relu', padding=\"same\"))\n",
    "model.add(MaxPooling2D(2))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Convolution2D(128, (2, 2),activation='relu', padding=\"same\"))\n",
    "model.add(MaxPooling2D(2))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Convolution2D(256, (2, 2),activation='relu', padding=\"same\"))\n",
    "\n",
    "model.add(GlobalAveragePooling2D())\n",
    "\n",
    "model.add(Dense(512,activation=\"relu\"))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Dense(256,activation=\"relu\"))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Output vector with 30 entries\n",
    "model.add(Dense(30))\n",
    "\n",
    "# Summarize the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import SGD, RMSprop, Adagrad, Adadelta, Adam, Adamax, Nadam\n",
    "\n",
    "## TODO: Compile the model\n",
    "model.compile(optimizer ='adam', loss='mse', metrics=['accuracy'])\n",
    "## TODO: Train the model\n",
    "hist = model.fit(np.reshape(X_train,[2140,96,96,1]), y_train, validation_split = 0.2, epochs = 10, batch_size=64, verbose=1)\n",
    "## TODO: Save the model as my_model.h5\n",
    "model.save('my_model_1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_test = model.predict(np.reshape(X_test,[1783,96,96,1]))\n",
    "fig = plt.figure(figsize=(20,20))\n",
    "fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)\n",
    "for i in range(9):\n",
    "    ax = fig.add_subplot(3, 3, i + 1, xticks=[], yticks=[])\n",
    "    plot_data(X_test[i], y_test[i], ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image = cv2.imread('obamas4.jpg')\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "def facial_keypoints_detector(image):\n",
    "    # Convert the RGB  image to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "    # Extract the pre-trained face detector from an xml file\n",
    "    face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "\n",
    "    # Detect the faces in image\n",
    "    faces = face_cascade.detectMultiScale(gray, 7, 7)\n",
    "\n",
    "    # Make a copy of the orginal image to draw face detections on\n",
    "    image_with_detections = np.copy(image)\n",
    "\n",
    "    fig = plt.figure(figsize = (8,8))\n",
    "    ax1 = fig.add_subplot(111)\n",
    "    ax1.set_xticks([])\n",
    "    ax1.set_yticks([])\n",
    "    ax1.set_title('Image with facial keypoints detection')\n",
    "    \n",
    "    # Get the bounding box for each detected face\n",
    "    for (x, y, w, h) in faces:\n",
    "        # Add a red bounding box to the detections image\n",
    "        cv2.rectangle(image_with_detections, (x, y), (x+w, y+h), (255, 0, 0), 3)\n",
    " \n",
    "        # Resize and reshape \n",
    "        face = gray[y:y+h, x:x+w] / 255\n",
    "        face = np.reshape(cv2.resize(face, (96, 96)), (-1, 96, 96, 1))\n",
    "\n",
    "        # Detect the key points of the face and plot them\n",
    "        predictions = model.predict(face)[0]\n",
    "        predictions = ((predictions + 1) * 0.5) * w\n",
    "        \n",
    "        ax1.scatter(predictions[0::2] + x, predictions[1::2] + y, marker='o', c='#00FF00', s=10)\n",
    "\n",
    "    # Display the image with the detections (hopefully... ;-)\n",
    "    ax1.imshow(image_with_detections)\n",
    "\n",
    "\n",
    "# The moment of the true has arrived (drum roll...)\n",
    "facial_keypoints_detector(image)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
